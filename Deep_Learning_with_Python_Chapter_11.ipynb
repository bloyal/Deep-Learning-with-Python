{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with Python Chapter 11.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BNXjpgNIjqUT"
      ],
      "authorship_tag": "ABX9TyN1Ny8yKAN/o+rjsAImer57",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bloyal/Deep-Learning-with-Python/blob/main/Deep_Learning_with_Python_Chapter_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNXjpgNIjqUT"
      },
      "source": [
        "# 11.2 Preparing text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPCmyz_BP_w3"
      },
      "source": [
        "# Creating a naive text vectorizer\n",
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "  \n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      #text = self.standardize(text)  # I think this is redundant\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    self.inverse_vocabulary = dict(\n",
        "        (v,k) for k, v in self.vocabulary.items()\n",
        "    )\n",
        "  \n",
        "  def encode(self, text):\n",
        "    #text = self.standardize(text) # I think this is redundant\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "  \n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(\n",
        "        self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence\n",
        "    )\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "           \"I write, erase, rewrite\",\n",
        "           \"Erase again, and then\",\n",
        "           \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOfIy1o-VF5Z",
        "outputId": "6d8cb608-7dd1-4259-8a8b-17985de2ceef"
      },
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(\"Encoded:\", encoded_sentence)\n",
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(\"Decoded:\", decoded_sentence)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded: [2, 3, 5, 7, 1, 5, 6]\n",
            "Decoded: i write rewrite and [UNK] rewrite again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pjmuVxsEVXrJ",
        "outputId": "0930b349-a60b-4f77-f244-49a65143bda4"
      },
      "source": [
        "vectorizer.standardize(test_sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'i write rewrite and still rewrite again'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDSv0dgBV0Fj",
        "outputId": "8c8a88a7-2218-4a99-efaf-266b7bd0ae2e"
      },
      "source": [
        "vectorizer.tokenize(test_sentence)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'write', 'rewrite', 'and', 'still', 'rewrite', 'again']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GSkrdoQV3CC",
        "outputId": "982ebfbe-02bb-4c37-c1c5-2a42341e565f"
      },
      "source": [
        "vectorizer.vocabulary"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0,\n",
              " '[UNK]': 1,\n",
              " 'a': 9,\n",
              " 'again': 6,\n",
              " 'and': 7,\n",
              " 'blooms': 11,\n",
              " 'erase': 4,\n",
              " 'i': 2,\n",
              " 'poppy': 10,\n",
              " 'rewrite': 5,\n",
              " 'then': 8,\n",
              " 'write': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PzKdAR2WDEb",
        "outputId": "370cb716-ac3a-4594-feb6-f48364cafbc7"
      },
      "source": [
        "vectorizer.inverse_vocabulary"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '',\n",
              " 1: '[UNK]',\n",
              " 2: 'i',\n",
              " 3: 'write',\n",
              " 4: 'erase',\n",
              " 5: 'rewrite',\n",
              " 6: 'again',\n",
              " 7: 'and',\n",
              " 8: 'then',\n",
              " 9: 'a',\n",
              " 10: 'poppy',\n",
              " 11: 'blooms'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6Uxw-nSWEv0"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "text_vectorization = TextVectorization(output_mode=\"int\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8QgY4bciGBf"
      },
      "source": [
        "dataset = [\n",
        "           \"I write, erase, rewrite\",\n",
        "           \"Erase again, and then\",\n",
        "           \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oz6bRTHiN7c",
        "outputId": "1a8f7691-29b5-4bf8-849c-240319108db8"
      },
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)\n",
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
            "i write rewrite and [UNK] rewrite again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8viu_ImTjxvD"
      },
      "source": [
        "# 11.3 Two approaches for representing groups of words: sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfB3u7qMkMen"
      },
      "source": [
        "## 11.3.1 Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8N3CbhZiQc8",
        "outputId": "4ef387e5-e0f8-4e0d-f97a-e1f26192da1b"
      },
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  62.6M      0  0:00:01  0:00:01 --:--:-- 62.6M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBVHL-EykQ5M"
      },
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDbWnjQckiMU",
        "outputId": "394e3e05-c64a-45ec-b0f4-011263fd0156"
      },
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC3HZafmkk8S"
      },
      "source": [
        "# Prepare a validation set in addition to test and train\n",
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuW68Q_xlRDS",
        "outputId": "5c8af4a4-74df-4125-c2a6-6e0bf0523bdc"
      },
      "source": [
        "# Created batched Datasets of text files for training, validation, and testing\n",
        "\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx1kR7zTljYf",
        "outputId": "81c3b443-b9f8-4937-8ff2-c92a89901284"
      },
      "source": [
        "# Listing 11.4 Displaying the shapes and dtypes of the first batch\n",
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)  \n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor(b\"Corbin Bernsen gives a terrifically intense and riveting performance as Dr. Alan Feinstone, a wealthy and successful Beverly Hills dentist who's obsessed with perfection. When he discovers that his lovely blonde babe trophy wife has been cheating on him and the IRS start hounding him about tax problems, Feinstone cracks under the pressure and goes violently around the bend. Director Brian Yuzna, working from a suitably dark, witty and demented script by Stuart Gordon, Dennis Paoli, and Charles Finch, exposes the seething neurosis and psychosis bubbling underneath the squeaky clean well-manicured surface of respectable affluent rich America with deliciously malicious glee. Moreover, Yuzna further spices up the grisly goings on with a wickedly twisted sense of pitch black gallows humor. Bernsen positively shines as Dr. Feinstone; he expertly projects a truly unnerving underlying creepiness that's right beneath Feinstone's deceptively calm and assured veneer. The supporting cast are likewise excellent: Linda Hoffman as Feinstone's bitchy, unfaithful wife Brooke, Earl Boen as smarmy, meddlesome IRS agent Marvin Goldblum, Molly Hagan as feisty assistant Jessica, Patty Toy as perky assistant Karen, Jan Hoag as jolly office manager Candy, Virginya Keehne as sweet, gawky teenager Sarah, Ken Foree as thorough, no-nonsense Detective Gibbs, Tony Noakes as Gibbs' equally shrewd partner Detective Sunshine, Michael Stadvec as womanizing stud muffin pool cleaner Matt, and Mark Ruffalo as on the make sleazeball Steve Landers. The first-rate make-up f/x are every bit as gory, gross and upsetting as they ought to be. The polished cinematography by Levie Isaaks boasts lots of great crazy tilted camera angles and a few tasty zoom-in close-ups. Alan Howarth's spirited shuddery score also hits the flesh-crawling spot. An enjoyably warped treat.\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVUkAXJ0mpug"
      },
      "source": [
        "## 11.3.2 Processing words as a set: the bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQe2Bwdhwnpv"
      },
      "source": [
        "### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpbSGC_oqClJ"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "text_vectorization = TextVectorization(output_mode=\"int\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apWX7_0HmOmU"
      },
      "source": [
        "# Listing 11.5 Preprocessing our datasets with a TextVectorization layer\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"binary\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRhBLGqyoUdS",
        "outputId": "39185e22-a6e3-40d8-be91-918ee8fb04d7"
      },
      "source": [
        "#Listing 11.6 Inspecting the output of our binary unigram dataset\n",
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]:\", inputs[0])\n",
        "  print(\"targets[0]:\", targets[0])\n",
        "  break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7-TRyrMooBF"
      },
      "source": [
        "# Listing 11.7 A reusable model-building utility\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QacNozDpjDc",
        "outputId": "0df47d36-3158-4bfe-a1bb-c8993263dd08"
      },
      "source": [
        "# Listing 11.8 Training and testing the binary unigram model\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                320016    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG1-kuzGppaY",
        "outputId": "e0759c33-3503-4322-95d2-cdbd1268dddf"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 15ms/step - loss: 0.4029 - accuracy: 0.8307 - val_loss: 0.2946 - val_accuracy: 0.8804\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.2687 - accuracy: 0.9016 - val_loss: 0.2968 - val_accuracy: 0.8872\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2367 - accuracy: 0.9182 - val_loss: 0.3063 - val_accuracy: 0.8878\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2170 - accuracy: 0.9270 - val_loss: 0.3186 - val_accuracy: 0.8896\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2094 - accuracy: 0.9306 - val_loss: 0.3294 - val_accuracy: 0.8902\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2042 - accuracy: 0.9362 - val_loss: 0.3492 - val_accuracy: 0.8908\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2013 - accuracy: 0.9363 - val_loss: 0.3494 - val_accuracy: 0.8920\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1966 - accuracy: 0.9369 - val_loss: 0.3646 - val_accuracy: 0.8920\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2037 - accuracy: 0.9368 - val_loss: 0.3672 - val_accuracy: 0.8890\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.1962 - accuracy: 0.9387 - val_loss: 0.3711 - val_accuracy: 0.8860\n",
            "782/782 [==============================] - 8s 11ms/step - loss: 0.2959 - accuracy: 0.8829\n",
            "Test acc: 0.883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsuE4NT6wseL"
      },
      "source": [
        "### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B_opGDQpuDH"
      },
      "source": [
        "# Listing 11.9 Configuring the TextVectorization layer to return bigrams\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"binary\"\n",
        ")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyeEoOevw__a",
        "outputId": "5d52abfa-cbb2-41a8-e383-cf70f8e97560"
      },
      "source": [
        "# Listing 11.10 Training and testing the binary bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 16)                320016    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjRvqzCZxGeq",
        "outputId": "083d2a10-1b23-4851-d009-a08690e82b0d"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 18s 27ms/step - loss: 0.3758 - accuracy: 0.8458 - val_loss: 0.2666 - val_accuracy: 0.8974\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2486 - accuracy: 0.9140 - val_loss: 0.2711 - val_accuracy: 0.8996\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2136 - accuracy: 0.9303 - val_loss: 0.3042 - val_accuracy: 0.8948\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1960 - accuracy: 0.9384 - val_loss: 0.3282 - val_accuracy: 0.8962\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1867 - accuracy: 0.9412 - val_loss: 0.3412 - val_accuracy: 0.8950\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1875 - accuracy: 0.9438 - val_loss: 0.3571 - val_accuracy: 0.8902\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1905 - accuracy: 0.9455 - val_loss: 0.3642 - val_accuracy: 0.8918\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1823 - accuracy: 0.9469 - val_loss: 0.3675 - val_accuracy: 0.8922\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1812 - accuracy: 0.9489 - val_loss: 0.3789 - val_accuracy: 0.8880\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.1827 - accuracy: 0.9472 - val_loss: 0.3830 - val_accuracy: 0.8888\n",
            "782/782 [==============================] - 10s 13ms/step - loss: 0.2654 - accuracy: 0.8974\n",
            "Test acc: 0.897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkPmeJhxPgK"
      },
      "source": [
        "### Bigrams with TF-IDF encoding"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QySB849Uxiqf"
      },
      "source": [
        "# Listing 11.11 Configuring the TextVectorization layer to return token counts\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26A4gBqCxvK_"
      },
      "source": [
        "# Listing 11.12 Configuring the TextVectorization layer to return\n",
        "# TF-IDF-weighted outputs\n",
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf-idf\",\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS6Je0pGyUSY",
        "outputId": "1bfe461e-a006-4461-d9d4-0698f3bbd0e0"
      },
      "source": [
        "# Listing 11.13 Training and testing the TF-IDF bigram model\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "model = get_model()\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 20000)]           0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                320016    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_31xMSZ9yhIt",
        "outputId": "4ff83ac3-da8d-4d0e-e975-ccda93a97bdc"
      },
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 15s 23ms/step - loss: 0.4767 - accuracy: 0.7879 - val_loss: 0.2985 - val_accuracy: 0.8822\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.3403 - accuracy: 0.8635 - val_loss: 0.3041 - val_accuracy: 0.8792\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3189 - accuracy: 0.8730 - val_loss: 0.3143 - val_accuracy: 0.8748\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.3031 - accuracy: 0.8748 - val_loss: 0.3269 - val_accuracy: 0.8794\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2978 - accuracy: 0.8757 - val_loss: 0.3519 - val_accuracy: 0.8604\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2843 - accuracy: 0.8810 - val_loss: 0.3658 - val_accuracy: 0.8446\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2787 - accuracy: 0.8860 - val_loss: 0.3689 - val_accuracy: 0.8570\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2667 - accuracy: 0.8875 - val_loss: 0.3670 - val_accuracy: 0.8718\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2541 - accuracy: 0.8927 - val_loss: 0.3862 - val_accuracy: 0.8678\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 4ms/step - loss: 0.2446 - accuracy: 0.8969 - val_loss: 0.4004 - val_accuracy: 0.8576\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.2976 - accuracy: 0.8804\n",
            "Test acc: 0.880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2raZfGdVyoZU"
      },
      "source": [
        "# Exporting a model that processes raw strings (i.e. incorporating the text\n",
        "# vectorization layer into the model)\n",
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dTuB985zgqm",
        "outputId": "4c9aa66a-19b2-4f2b-8de0-19d23a0dce83"
      },
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86.76 percent positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owv-ifpszzG4"
      },
      "source": [
        "## 11.3.3 Processing words as a sequence: the Sequence Model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DfwPhAx0Nco"
      },
      "source": [
        "### A first practical example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDXION5Qzxzf"
      },
      "source": [
        "# Listing 11.14 Preparing integer sequence datasets\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjkUNxzA0pr1",
        "outputId": "f8dd4654-291e-4083-bb0e-fd3369e25773"
      },
      "source": [
        "#Listing 11.5 A sequence model built on top of one-hot encoded vector sequences\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "tf.one_hot (TFOpLambda)      (None, None, 20000)       0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 64)                5128448   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juhZxBKh1sTn"
      },
      "source": [
        "# Listing 11.16 Training a first basic sequence model\n",
        "# Note: This takes forever because the one-hot encoded inputs are so large\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lff9SMfc3iD5"
      },
      "source": [
        "### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhNVsgp83kyx"
      },
      "source": [
        "#### Learning word embeddings with the `Embedding` layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMUZg7BX3gMT",
        "outputId": "ca90a858-6970-4a01-a8c9-5c90be27c245"
      },
      "source": [
        "# Listing 11.18 Model that uses an Embedding layer trained from scratch\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 64)                73984     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 57s 86ms/step - loss: 0.4543 - accuracy: 0.7958 - val_loss: 0.3253 - val_accuracy: 0.8698\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.2914 - accuracy: 0.8931 - val_loss: 0.3225 - val_accuracy: 0.8788\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.2297 - accuracy: 0.9182 - val_loss: 0.3572 - val_accuracy: 0.8812\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.1866 - accuracy: 0.9351 - val_loss: 0.3942 - val_accuracy: 0.8818\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.1565 - accuracy: 0.9480 - val_loss: 0.3716 - val_accuracy: 0.8778\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 55s 88ms/step - loss: 0.1318 - accuracy: 0.9571 - val_loss: 0.4037 - val_accuracy: 0.8760\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.1061 - accuracy: 0.9662 - val_loss: 0.4560 - val_accuracy: 0.8788\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 54s 87ms/step - loss: 0.0915 - accuracy: 0.9728 - val_loss: 0.4328 - val_accuracy: 0.8770\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 53s 85ms/step - loss: 0.0745 - accuracy: 0.9776 - val_loss: 0.4432 - val_accuracy: 0.8804\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 54s 86ms/step - loss: 0.0630 - accuracy: 0.9807 - val_loss: 0.4721 - val_accuracy: 0.8770\n",
            "782/782 [==============================] - 30s 37ms/step - loss: 0.3571 - accuracy: 0.8624\n",
            "Test acc: 0.862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RziTgM966x07"
      },
      "source": [
        "### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ole9UgJS4zdb",
        "outputId": "e5d8438d-9487-4198-ca33-4fd0835b6b33"
      },
      "source": [
        "# Listing 1.19 Model that uses an Embedding layer trained from scratch, with masking enabledr\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_9 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 64)                73984     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 66s 95ms/step - loss: 0.3822 - accuracy: 0.8262 - val_loss: 0.2823 - val_accuracy: 0.8870\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.2232 - accuracy: 0.9165 - val_loss: 0.3264 - val_accuracy: 0.8750\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 57s 92ms/step - loss: 0.1655 - accuracy: 0.9399 - val_loss: 0.4114 - val_accuracy: 0.8530\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.1242 - accuracy: 0.9552 - val_loss: 0.3678 - val_accuracy: 0.8810\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.0944 - accuracy: 0.9658 - val_loss: 0.3680 - val_accuracy: 0.8742\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.0693 - accuracy: 0.9759 - val_loss: 0.4683 - val_accuracy: 0.8814\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 57s 92ms/step - loss: 0.0486 - accuracy: 0.9847 - val_loss: 0.4366 - val_accuracy: 0.8716\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 58s 92ms/step - loss: 0.0349 - accuracy: 0.9888 - val_loss: 0.4929 - val_accuracy: 0.8760\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.5464 - val_accuracy: 0.8758\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 57s 91ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.6834 - val_accuracy: 0.8726\n",
            "782/782 [==============================] - 27s 33ms/step - loss: 0.2852 - accuracy: 0.8796\n",
            "Test acc: 0.880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbjyfoXVHzjM"
      },
      "source": [
        "### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1eSPyNs7VbP",
        "outputId": "e8ea781c-223e-41df-d811-cc0789d88837"
      },
      "source": [
        "# Download the GloVe word embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-10 17:35:43--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-08-10 17:35:43--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-08-10 17:35:43--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.11MB/s    in 2m 39s  \n",
            "\n",
            "2021-08-10 17:38:22 (5.16 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBGoYmEnIlGx",
        "outputId": "728dc196-1ebe-457f-bded-e9a5813cdcfd"
      },
      "source": [
        "!head glove.6B.100d.txt"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
            ", -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\n",
            ". -0.33979 0.20941 0.46348 -0.64792 -0.38377 0.038034 0.17127 0.15978 0.46619 -0.019169 0.41479 -0.34349 0.26872 0.04464 0.42131 -0.41032 0.15459 0.022239 -0.64653 0.25256 0.043136 -0.19445 0.46516 0.45651 0.68588 0.091295 0.21875 -0.70351 0.16785 -0.35079 -0.12634 0.66384 -0.2582 0.036542 -0.13605 0.40253 0.14289 0.38132 -0.12283 -0.45886 -0.25282 -0.30432 -0.11215 -0.26182 -0.22482 -0.44554 0.2991 -0.85612 -0.14503 -0.49086 0.0082973 -0.17491 0.27524 1.4401 -0.21239 -2.8435 -0.27958 -0.45722 1.6386 0.78808 -0.55262 0.65 0.086426 0.39012 1.0632 -0.35379 0.48328 0.346 0.84174 0.098707 -0.24213 -0.27053 0.045287 -0.40147 0.11395 0.0062226 0.036673 0.018518 -1.0213 -0.20806 0.64072 -0.068763 -0.58635 0.33476 -1.1432 -0.1148 -0.25091 -0.45907 -0.096819 -0.17946 -0.063351 -0.67412 -0.068895 0.53604 -0.87773 0.31802 -0.39242 -0.23394 0.47298 -0.028803\n",
            "of -0.1529 -0.24279 0.89837 0.16996 0.53516 0.48784 -0.58826 -0.17982 -1.3581 0.42541 0.15377 0.24215 0.13474 0.41193 0.67043 -0.56418 0.42985 -0.012183 -0.11677 0.31781 0.054177 -0.054273 0.35516 -0.30241 0.31434 -0.33846 0.71715 -0.26855 -0.15837 -0.47467 0.051581 -0.33252 0.15003 -0.1299 -0.54617 -0.37843 0.64261 0.82187 -0.080006 0.078479 -0.96976 -0.57741 0.56491 -0.39873 -0.057099 0.19743 0.065706 -0.48092 -0.20125 -0.40834 0.39456 -0.02642 -0.11838 1.012 -0.53171 -2.7474 -0.042981 -0.74849 1.7574 0.59085 0.04885 0.78267 0.38497 0.42097 0.67882 0.10337 0.6328 -0.026595 0.58647 -0.44332 0.33057 -0.12022 -0.55645 0.073611 0.20915 0.43395 -0.012761 0.089874 -1.7991 0.084808 0.77112 0.63105 -0.90685 0.60326 -1.7515 0.18596 -0.50687 -0.70203 0.66578 -0.81304 0.18712 -0.018488 -0.26757 0.727 -0.59363 -0.34839 -0.56094 -0.591 1.0039 0.20664\n",
            "to -0.1897 0.050024 0.19084 -0.049184 -0.089737 0.21006 -0.54952 0.098377 -0.20135 0.34241 -0.092677 0.161 -0.13268 -0.2816 0.18737 -0.42959 0.96039 0.13972 -1.0781 0.40518 0.50539 -0.55064 0.4844 0.38044 -0.0029055 -0.34942 -0.099696 -0.78368 1.0363 -0.2314 -0.47121 0.57126 -0.21454 0.35958 -0.48319 1.0875 0.28524 0.12447 -0.039248 -0.076732 -0.76343 -0.32409 -0.5749 -1.0893 -0.41811 0.4512 0.12112 -0.51367 -0.13349 -1.1378 -0.28768 0.16774 0.55804 1.5387 0.018859 -2.9721 -0.24216 -0.92495 2.1992 0.28234 -0.3478 0.51621 -0.43387 0.36852 0.74573 0.072102 0.27931 0.92569 -0.050336 -0.85856 -0.1358 -0.92551 -0.33991 -1.0394 -0.067203 -0.21379 -0.4769 0.21377 -0.84008 0.052536 0.59298 0.29604 -0.67644 0.13916 -1.5504 -0.20765 0.7222 0.52056 -0.076221 -0.15194 -0.13134 0.058617 -0.31869 -0.61419 -0.62393 -0.41548 -0.038175 -0.39804 0.47647 -0.15983\n",
            "and -0.071953 0.23127 0.023731 -0.50638 0.33923 0.1959 -0.32943 0.18364 -0.18057 0.28963 0.20448 -0.5496 0.27399 0.58327 0.20468 -0.49228 0.19974 -0.070237 -0.88049 0.29485 0.14071 -0.1009 0.99449 0.36973 0.44554 0.28998 -0.1376 -0.56365 -0.029365 -0.4122 -0.25269 0.63181 -0.44767 0.24363 -0.10813 0.25164 0.46967 0.3755 -0.23613 -0.14129 -0.44537 -0.65737 -0.042421 -0.28636 -0.28811 0.063766 0.20281 -0.53542 0.41307 -0.59722 -0.38614 0.19389 -0.17809 1.6618 -0.011819 -2.3737 0.058427 -0.2698 1.2823 0.81925 -0.22322 0.72932 -0.053211 0.43507 0.85011 -0.42935 0.92664 0.39051 1.0585 -0.24561 -0.18265 -0.5328 0.059518 -0.66019 0.18991 0.28836 -0.2434 0.52784 -0.65762 -0.14081 1.0491 0.5134 -0.23816 0.69895 -1.4813 -0.2487 -0.17936 -0.059137 -0.08056 -0.48782 0.014487 -0.6259 -0.32367 0.41862 -1.0807 0.46742 -0.49931 -0.71895 0.86894 0.19539\n",
            "in 0.085703 -0.22201 0.16569 0.13373 0.38239 0.35401 0.01287 0.22461 -0.43817 0.50164 -0.35874 -0.34983 0.055156 0.69648 -0.17958 0.067926 0.39101 0.16039 -0.26635 -0.21138 0.53698 0.49379 0.9366 0.66902 0.21793 -0.46642 0.22383 -0.36204 -0.17656 0.1748 -0.20367 0.13931 0.019832 -0.10413 -0.20244 0.55003 -0.1546 0.98655 -0.26863 -0.2909 -0.32866 -0.34188 -0.16943 -0.42001 -0.046727 -0.16327 0.70824 -0.74911 -0.091559 -0.96178 -0.19747 0.10282 0.55221 1.3816 -0.65636 -3.2502 -0.31556 -1.2055 1.7709 0.4026 -0.79827 1.1597 -0.33042 0.31382 0.77386 0.22595 0.52471 -0.034053 0.32048 0.079948 0.17752 -0.49426 -0.70045 -0.44569 0.17244 0.20278 0.023292 -0.20677 -1.0158 0.18325 0.56752 0.31821 -0.65011 0.68277 -0.86585 -0.059392 -0.29264 -0.55668 -0.34705 -0.32895 0.40215 -0.12746 -0.20228 0.87368 -0.545 0.79205 -0.20695 -0.074273 0.75808 -0.34243\n",
            "a -0.27086 0.044006 -0.02026 -0.17395 0.6444 0.71213 0.3551 0.47138 -0.29637 0.54427 -0.72294 -0.0047612 0.040611 0.043236 0.29729 0.10725 0.40156 -0.53662 0.033382 0.067396 0.64556 -0.085523 0.14103 0.094539 0.74947 -0.194 -0.68739 -0.41741 -0.22807 0.12 -0.48999 0.80945 0.045138 -0.11898 0.20161 0.39276 -0.20121 0.31354 0.75304 0.25907 -0.11566 -0.029319 0.93499 -0.36067 0.5242 0.23706 0.52715 0.22869 -0.51958 -0.79349 -0.20368 -0.50187 0.18748 0.94282 -0.44834 -3.6792 0.044183 -0.26751 2.1997 0.241 -0.033425 0.69553 -0.64472 -0.0072277 0.89575 0.20015 0.46493 0.61933 -0.1066 0.08691 -0.4623 0.18262 -0.15849 0.020791 0.19373 0.063426 -0.31673 -0.48177 -1.3848 0.13669 0.96859 0.049965 -0.2738 -0.035686 -1.0577 -0.24467 0.90366 -0.12442 0.080776 -0.83401 0.57201 0.088945 -0.42532 -0.018253 -0.079995 -0.28581 -0.01089 -0.4923 0.63687 0.23642\n",
            "\" -0.30457 -0.23645 0.17576 -0.72854 -0.28343 -0.2564 0.26587 0.025309 -0.074775 -0.3766 -0.057774 0.12159 0.34384 0.41928 -0.23236 -0.31547 0.60939 0.25117 -0.68667 0.70873 1.2162 -0.1824 -0.48442 -0.33445 0.30343 1.086 0.49992 -0.20198 0.27959 0.68352 -0.33566 -0.12405 0.059656 0.33617 0.37501 0.56552 0.44867 0.11284 -0.16196 -0.94346 -0.67961 0.18581 0.060653 0.43776 0.13834 -0.48207 -0.56141 -0.25422 -0.52445 0.097003 -0.48925 0.19077 0.21481 1.4969 -0.86665 -3.2846 0.56854 0.41971 1.2294 0.78522 -0.29369 0.63803 -1.5926 -0.20437 1.5306 0.13548 0.50722 0.18742 0.48552 -0.28995 0.19573 0.0046515 0.092879 -0.42444 0.64987 0.52839 0.077908 0.8263 -1.2208 -0.34955 0.49855 -0.64155 -0.72308 0.26566 -1.3643 -0.46364 -0.52048 -1.0525 0.22895 -0.3456 -0.658 -0.16735 0.35158 0.74337 0.26074 0.061104 -0.39079 -0.84557 -0.035432 0.17036\n",
            "'s 0.58854 -0.2025 0.73479 -0.68338 -0.19675 -0.1802 -0.39177 0.34172 -0.60561 0.63816 -0.26695 0.36486 -0.40379 -0.1134 -0.58718 0.2838 0.8025 -0.35303 0.30083 0.078935 0.44416 -0.45906 0.79294 0.50365 0.32805 0.28027 -0.4933 -0.38482 -0.039284 -0.2483 -0.1988 1.1469 0.13228 0.91691 -0.36739 0.89425 0.5426 0.61738 -0.62205 -0.31132 -0.50933 0.23335 1.0826 -0.044637 -0.12767 0.27628 -0.032617 -0.27397 0.77764 -0.50861 0.038307 -0.33679 0.42344 1.2271 -0.53826 -3.2411 0.42626 0.025189 1.3948 0.65085 0.03325 0.37141 0.4044 0.35558 0.98265 -0.61724 0.53901 0.76219 0.30689 0.33065 0.30956 -0.15161 -0.11313 -0.81281 0.6145 -0.44341 -0.19163 -0.089551 -1.5927 0.37405 0.85857 0.54613 -0.31928 0.52598 -1.4802 -0.97931 -0.2939 -0.14724 0.25803 -0.1817 1.0149 0.77649 0.12598 0.54779 -1.0316 0.064599 -0.37523 -0.94475 0.61802 0.39591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CSJwH_HJd6u",
        "outputId": "f8ff8a6d-5587-49b0-fff0-9cc8a25e755d"
      },
      "source": [
        "# Listing 11.20 Parsing the GloVe word-embeddings file\n",
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js-u3Q9ZKaWA"
      },
      "source": [
        "### Loading the GloVe embeddings in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zZOslN_KRZf"
      },
      "source": [
        "# Listing 11.21 Preparing the GloVe word-embeddings matrix\n",
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gIJW3b_KsBQ"
      },
      "source": [
        "# Load the pretrained embedding values into an Embedding layer\n",
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1uQBuv1L7Ld"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZnGKCfMSbt"
      },
      "source": [
        "### Training a simple bidirectional LSTM on top of the GloVE embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "260w1YSCMWQb",
        "outputId": "758ff2f2-a993-417b-81e0-e48f540a09e5"
      },
      "source": [
        "# Listing 11.22 Model that uses a pretrained Embedding layer\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 64)                34048     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 41s 55ms/step - loss: 0.5629 - accuracy: 0.7054 - val_loss: 0.5185 - val_accuracy: 0.7430\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.4490 - accuracy: 0.7972 - val_loss: 0.4268 - val_accuracy: 0.7990\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.3971 - accuracy: 0.8262 - val_loss: 0.3933 - val_accuracy: 0.8278\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3671 - accuracy: 0.8432 - val_loss: 0.4013 - val_accuracy: 0.8150\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.3485 - accuracy: 0.8512 - val_loss: 0.3505 - val_accuracy: 0.8498\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.3240 - accuracy: 0.8633 - val_loss: 0.3469 - val_accuracy: 0.8496\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.3022 - accuracy: 0.8732 - val_loss: 0.3542 - val_accuracy: 0.8592\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2870 - accuracy: 0.8814 - val_loss: 0.3316 - val_accuracy: 0.8706\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2737 - accuracy: 0.8881 - val_loss: 0.3044 - val_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.2610 - accuracy: 0.8949 - val_loss: 0.3028 - val_accuracy: 0.8772\n",
            "782/782 [==============================] - 23s 27ms/step - loss: 0.2873 - accuracy: 0.8809\n",
            "Test acc: 0.881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R3nYGUzePHY"
      },
      "source": [
        "# 11.4 The Transformer architecture (Dum dum dum)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCRLsWgqeOEq"
      },
      "source": [
        "# Listing 11.23 Transformer encoder implemented as a subclassed Layer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh10VnmBtQMH",
        "outputId": "1cde5618-4283-4a4f-a745-27e19938accb"
      },
      "source": [
        "# Listing 11.24 Text classification model that combines the Transformer encoder and a pooling layer\n",
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 256)         5120000   \n",
            "_________________________________________________________________\n",
            "transformer_encoder (Transfo (None, None, 256)         543776    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 5,664,033\n",
            "Trainable params: 5,664,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "MoT1nJatwS7Z",
        "outputId": "cd1b6dbe-87ec-4a17-abcb-09f53f2ae92f"
      },
      "source": [
        "# Listing 11.25 Training and evaluating the Transformer encoder based model\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "625/625 [==============================] - 66s 95ms/step - loss: 0.4871 - accuracy: 0.7717 - val_loss: 0.3246 - val_accuracy: 0.8578\n",
            "Epoch 2/20\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.3159 - accuracy: 0.8672 - val_loss: 0.2781 - val_accuracy: 0.8854\n",
            "Epoch 3/20\n",
            "625/625 [==============================] - 59s 94ms/step - loss: 0.2452 - accuracy: 0.8995 - val_loss: 0.2780 - val_accuracy: 0.8840\n",
            "Epoch 4/20\n",
            "625/625 [==============================] - 59s 94ms/step - loss: 0.1915 - accuracy: 0.9251 - val_loss: 0.2872 - val_accuracy: 0.8836\n",
            "Epoch 5/20\n",
            "625/625 [==============================] - 59s 94ms/step - loss: 0.1563 - accuracy: 0.9402 - val_loss: 0.2941 - val_accuracy: 0.8846\n",
            "Epoch 6/20\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.1293 - accuracy: 0.9525 - val_loss: 0.3410 - val_accuracy: 0.8876\n",
            "Epoch 7/20\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.1117 - accuracy: 0.9590 - val_loss: 0.4624 - val_accuracy: 0.8726\n",
            "Epoch 8/20\n",
            "625/625 [==============================] - 58s 93ms/step - loss: 0.0943 - accuracy: 0.9671 - val_loss: 0.3926 - val_accuracy: 0.8568\n",
            "Epoch 9/20\n",
            "221/625 [=========>....................] - ETA: 34s - loss: 0.0838 - accuracy: 0.9707"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5abd85e48505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                     save_best_only=True)\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m model = keras.models.load_model(\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"transformer_encoder.keras\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1RhctAtxDNW"
      },
      "source": [
        "# Listing 11.26 Implementing positional embedding as a subclassed layer\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmv0x67ZzrG_"
      },
      "source": [
        "### Putting it all together: A text-classification Transformer\n",
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB2X39-61S4m"
      },
      "source": [
        "# 11.5 Beyond text classification: sequence-to-sequence learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qiqi6Isz5gG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}